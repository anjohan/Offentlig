\documentclass[12pt,english,a4paper]{report}
\pdfobjcompresslevel=0
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[includeheadfoot,margin=0.8 in,top=0.6 in]{geometry}
\usepackage{siunitx,physics,cancel,upgreek,varioref,listings,booktabs,tocloft, pdfpages}
\usepackage{mathtools}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{fouriernc}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{lastpage}
\usepackage{microtype}
\usepackage[linktoc=all, bookmarks=true, pdfauthor={Anders Johansson}]{hyperref}
\renewcommand{\CancelColor}{\color{red}}
\renewcommand{\exp}[1]{\mathrm{e}^{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\tittel}[1]{\title{#1 \vspace{-7ex}}\author{}\date{}\maketitle\thispagestyle{fancy}\pagestyle{fancy}\setcounter{page}{1}}

\newcommand{\deloppg}[2][]{\subsection*{#2) #1}\addcontentsline{toc}{subsection}{#2)}\refstepcounter{subsection}\label{#2}}
\newcommand{\oppg}[1]{\section*{Oppgave #1}\addcontentsline{toc}{section}{Oppgave #1}\refstepcounter{section}\label{oppg#1}}

\labelformat{section}{section~#1}
\labelformat{subsection}{section~#1}
\labelformat{subsubsection}{paragraph~#1}
\labelformat{equation}{equation~(#1)}
\labelformat{figure}{figure~#1}
\labelformat{table}{table~#1}

\lstset{rangeprefix=/*\#,
rangesuffix=\#*/,
includerangemarker=false}
\renewcommand{\lstlistingname}{Code snippet}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstset{showstringspaces=false,
basicstyle=\footnotesize\ttfamily,
keywordstyle=\color{codegreen},
commentstyle=\color{magenta},
numberstyle=\tiny\color{codegray},
stringstyle=\color{codepurple},
frameshape={RYRYNYYYY}{yny}{yny}{RYRYNYYYY},
breaklines=true,
literate={0}{{\textcolor{blue}{0}}}{1}%
             {1}{{\textcolor{blue}{1}}}{1}%
             {2}{{\textcolor{blue}{2}}}{1}%
             {3}{{\textcolor{blue}{3}}}{1}%
             {4}{{\textcolor{blue}{4}}}{1}%
             {5}{{\textcolor{blue}{5}}}{1}%
             {6}{{\textcolor{blue}{6}}}{1}%
             {7}{{\textcolor{blue}{7}}}{1}%
             {8}{{\textcolor{blue}{8}}}{1}%
             {9}{{\textcolor{blue}{9}}}{1}%
             {.0}{{\textcolor{blue}{.0}}}{2}% Following is to ensure that only periods
             {.1}{{\textcolor{blue}{.1}}}{2}% followed by a digit are changed.
             {.2}{{\textcolor{blue}{.2}}}{2}%
             {.3}{{\textcolor{blue}{.3}}}{2}%
             {.4}{{\textcolor{blue}{.4}}}{2}%
             {.5}{{\textcolor{blue}{.5}}}{2}%
             {.6}{{\textcolor{blue}{.6}}}{2}%
             {.7}{{\textcolor{blue}{.7}}}{2}%
             {.8}{{\textcolor{blue}{.8}}}{2}%
             {.9}{{\textcolor{blue}{.9}}}{2}%
}

\renewcommand{\footrulewidth}{\headrulewidth}
\tocloftpagestyle{fancy}

\setcounter{secnumdepth}{4}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\arabic{section}.\arabic{subsection}.\arabic{subsubsection}}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1em}

\newcommand{\eqtag}[1]{\refstepcounter{equation}\tag{\theequation}\label{#1}}
\hypersetup{colorlinks=true,urlcolor=blue,linkcolor=black}

\sisetup{detect-all}
\sisetup{exponent-product = \cdot, output-product = \cdot,per-mode=symbol}
\sisetup{output-decimal-marker={.}}
\sisetup{round-mode = off, round-precision=3}
\sisetup{number-unit-product = \ }

\allowdisplaybreaks[4]
\fancyhf{}

\rhead{Anders Johansson}
\rfoot{Page \thepage{} of \pageref{LastPage}}
\lhead{FYS3150}
%
\usepackage[backend=biber,citestyle=numeric-comp,bibstyle=numeric,sorting=none]{biblatex}
\DefineBibliographyStrings{norsk}{%
  bibliography = {Referanser},
}
\DefineBibliographyStrings{english}{%
  bibliography = {References},
}
\addbibresource{kilder.bib}

\begin{document}
\includepdf{forside.pdf}
\pagestyle{fancy}
\tableofcontents

%      _    _         _                  _
%     / \  | |__  ___| |_ _ __ __ _  ___| |_
%    / _ \ | '_ \/ __| __| '__/ _` |/ __| __|
%   / ___ \| |_) \__ \ |_| | | (_| | (__| |_
%  /_/   \_\_.__/|___/\__|_|  \__,_|\___|\__|
%

\section{Abstract}

All files for this project are available at GitHub\footnote{\url{https://github.com/anjohan/Offentlig/tree/master/FYS3150/Oblig2}}.

In this project, Jacobi's method for finding eigenvalues and eigenvectors is derived, implemented and applied to Schrödinger's equation for both one and two electrons in a harmonic oscillator well, in the latter case both with and without Coulomb interaction between the particles.

Jacobi's method proved to be quite slow even for small systems (e.g. with \(400\) steps), but it was successful in finding the energies and eigenstates for the harmonic oscillator. For the tridiagonal matrix which results from the three point approximation to the second derivative, the number of required rotations was proportional to the square of the number of steps, while the time was roughly proportional to the number of steps to the power of \(4\).

%   ___       _                 _            _   _
%  |_ _|_ __ | |_ _ __ ___   __| |_   _  ___| |_(_) ___  _ __
%   | || '_ \| __| '__/ _ \ / _` | | | |/ __| __| |/ _ \| '_ \
%   | || | | | |_| | | (_) | (_| | |_| | (__| |_| | (_) | | | |
%  |___|_| |_|\__|_|  \___/ \__,_|\__,_|\___|\__|_|\___/|_| |_|
%

\section{Introduction}
Differential equations play a vital role in physics, as many of the central laws and equations, such as Newtons laws and Schrödinger's equations, are formulated as differential equations. The majority of these equations are difficult, if not impossible, to solve analytically. As a result, methods for numerical solution of differential equations are important tools for physicists.

Often, physical laws lead to differential equations where the derivative of some order of a function depends on the function itself. If the dependence is linear, discretisation of the function leads to an eigenvalue problem, which in most cases must be solved numerically.

The goal of this report is to implement Jacobi's algorithm , which is an iterative scheme for finding the eigenvalues and eigenvectors of a matrix, and to use this to solve Schrödinger's equation.

The first part of the report deals with both the physical and the mathematical theory of the problem. In the physical part, Schrödinger's equation is rewritten to a simpler form with dimensionless variables. This is followed by the mathematical part where properties of orthogonal transformations and orthogonal similarity transformations are summarized and in some cases proved, and then used to derive Jacobi's algorithm. The algorithm is then implemented in C++. The implementation is object oriented and utilises polymorphism, so it can be used to solve Schrödinger's equation for any potential. Two tests for the algorithm are also included.

Later in the report, the implementation is applied to Schrödinger's equation for both one and two particles in a harmonic oscillator well, the latter both with and without Coulomb interaction. The required number of iterations and time is informally analysed, while the resulting eigenstates are given a physical interpretation.



%  ____  _               _           _   _   _
% |  _ \| |__  _   _ ___(_) ___ __ _| | | |_| |__   ___  ___  _ __ _   _
% | |_) | '_ \| | | / __| |/ __/ _` | | | __| '_ \ / _ \/ _ \| '__| | | |
% |  __/| | | | |_| \__ \ | (_| (_| | | | |_| | | |  __/ (_) | |  | |_| |
% |_|   |_| |_|\__, |___/_|\___\__,_|_|  \__|_| |_|\___|\___/|_|   \__, |
%              |___/                                               |___/
\clearpage
\section{Physical theory}
This project deals with the famous Schrödinger equation for various systems. A detailed analytical solution of the problems can be found in \autocite{griffiths}.

%    ___                                _   _      _
%   / _ \ _ __   ___   _ __   __ _ _ __| |_(_) ___| | ___
%  | | | | '_ \ / _ \ | '_ \ / _` | '__| __| |/ __| |/ _ \
%  | |_| | | | |  __/ | |_) | (_| | |  | |_| | (__| |  __/
%   \___/|_| |_|\___| | .__/ \__,_|_|   \__|_|\___|_|\___|
%                     |_|

\subsection{One particle in a harmonic oscillator}
The time independent form of the Schrödinger equation for one particle is
\[
-\frac{\hbar^2}{2m}\nabla^2\psi(\vec{r}) + V(\vec{r})\psi(\vec{r}) = E\psi(\vec{r}) \eqtag{genslen}
\]
where \(V\) is a potential and \(\psi\) is the wave function of the particle. According to Max Born's interpretation, the absolute square of the wave function is a probability distribution, meaning that the probability of finding the particle in a specific area is equal to the integral of the absolute square of the wave function over that area. One consequence is that if the integral is taken over all of space, the result must be \(1\).

If the potential is centrosymmetric, one can assume that the wave function is the product of one centrosymmetric and one angle-dependent function. This leads to a separation of variables, where the angle-dependent equations have the spherical harmonics as solutions. The radial equation is
\[
-\frac{\hbar^2}{2m}\qty(\frac{1}{r^2}\dv{r}(r^2\dv{R(r)}{r}) - \frac{\ell\qty(\ell+1)}{r^2}R(r)) + V(r)R(r)=ER(r)
\]
If  we look at the states where the orbital quantum number \(\ell\) is \(0\) and introduce the function \(u(r)=rR(r)\), this can be simplified to
\begin{alignat*}{2}
-\frac{\hbar^2}{2m}\dv[2]{u(r)}{r} + V(r)u(r) &= Eu(r)
\intertext{A harmonic oscillator means that the potential is given by \(V(r)=\tfrac{1}{2}kr^2\). Introducing the variable \(\rho=r/\alpha\) with \(\alpha=\qty(\hbar^2/mk)^{1/4}\) and defining \(\lambda=2m\alpha^2E/\hbar^2\), we arrive at the final equation}
-\dv[2]{u(\rho)}{\rho}+\rho^2u(\rho)&=\lambda u(\rho)\eqtag{slen}
\end{alignat*}
It is known\autocite{compphys} that this equation has \(\lambda=3,7,11,\dots\).

%   _____                                  _   _      _
%  |_   _|_      _____    _ __   __ _ _ __| |_(_) ___| | ___  ___
%    | | \ \ /\ / / _ \  | '_ \ / _` | '__| __| |/ __| |/ _ \/ __|
%    | |  \ V  V / (_) | | |_) | (_| | |  | |_| | (__| |  __/\__ \
%    |_|   \_/\_/ \___/  | .__/ \__,_|_|   \__|_|\___|_|\___||___/
%                        |_|


\subsection{Two particles}
For two particles in a harmonic oscillator potential, the Schrödinger equation can be written as
\[
\qty(-\frac{\hbar^2}{2m}\dv[2]{r_1} + \tfrac{1}{2}kr_1^2 - \frac{\hbar^2}{2m}\dv[2]{r_2} + \tfrac{1}{2}kr_2^2)u(r_1,r_2) = Eu(r_1,r_2)
\]
Using a similar approach as in the case with one particle, this can be rewritten as
\begin{alignat*}{2}
-\dv[2]{\psi(\rho)}{\rho} + \omega_r^2\rho^2\psi(\rho) &= \lambda\psi(\rho) \eqtag{slutento}
\intertext{where \(\rho\) is the distance between the particles. There is also an equation for the position of the centre of mass for the system consisting of the two particles, which will not be studied here. If we include the Coulomb interaction between the electrons, the equation instead takes the form}
-\dv[2]{\psi(\rho)}{\rho} + \omega_r^2\rho^2\psi(\rho) + \frac{1}{\rho} &= \lambda\psi(\rho) \eqtag{slmedto}
\end{alignat*}

%   _____                     __                            _   _               _
%  |_   _| __ __ _ _ __  ___ / _| ___  _ __ _ __ ___   __ _| |_(_) ___  _ __   | |_ ___     __ _ _ __
%    | || '__/ _` | '_ \/ __| |_ / _ \| '__| '_ ` _ \ / _` | __| |/ _ \| '_ \  | __/ _ \   / _` | '_ \
%    | || | | (_| | | | \__ \  _| (_) | |  | | | | | | (_| | |_| | (_) | | | | | || (_) | | (_| | | | |
%    |_||_|  \__,_|_| |_|___/_|  \___/|_|  |_| |_| |_|\__,_|\__|_|\___/|_| |_|  \__\___/   \__,_|_| |_|
%    ___(_) __ _  ___ _ ____   ____ _| |_   _  ___   _ __  _ __ ___ | |__ | | ___ _ __ ___
%   / _ \ |/ _` |/ _ \ '_ \ \ / / _` | | | | |/ _ \ | '_ \| '__/ _ \| '_ \| |/ _ \ '_ ` _ \
%  |  __/ | (_| |  __/ | | \ V / (_| | | |_| |  __/ | |_) | | | (_) | |_) | |  __/ | | | | |
%   \___|_|\__, |\___|_| |_|\_/ \__,_|_|\__,_|\___| | .__/|_|  \___/|_.__/|_|\___|_| |_| |_|
%          |___/                                    |_|

\subsection{Transformation to an eigenvalue problem}
\Ref{slen}, \ref{slutento} and \ref{slmedto} can all be written on the generic form
\[
-\dv[2]{\psi(\rho)}{\rho} + V(\rho)\psi(\rho) = \lambda\psi(\rho)
\]
where \(V(\rho)\) is either \(\rho^2\), \(\omega_r^2\rho^2\) or \(\omega_r^2\rho^2+1/\rho\) depending on the situation.

The wave function must be normalized, which requires that the integral of the absolute square of the function over all of space must be \(1\). Because \(\rho\) is a distance and therefore positive, all of space translates to \([0,\infty)\). The particles can not be on top of each other, so we must have \(\psi_0=0\), while normalizability requires that \(\psi_{n+1}=\psi(\infty)=0\). This gives the boundary conditions of the problem.

As discussed in \autocite{oblig1}, this equation can be discretised using the three point approximation of the second derivative:
\[
\psi_i'' = \frac{\psi_{i+1} + \psi_{i-1} - 2\psi_i}{h^2} + O(h^2)
\]
Inserting this into the equation above, we get
\[
-\frac{\psi_{i+1} + \psi_{i-1} - 2\psi_i}{h^2} + V_i\psi_i = \lambda\psi_i
\]
where \(V_i=V(\rho_i)\). We know from the boundary conditions that \(\psi_0=\psi_{n+1}=0\), so the set of equations for the remaining \(n\) points can be written on a matrix form as
\[
\begin{bmatrix}
\frac{2}{h^2} + V_1 & -\frac{1}{h^2} & 0 & 0 & 0 & \cdots & 0 & 0\\
-\frac{1}{h^2} & \frac{2}{h^2} + V_2 & -\frac{1}{h^2} & 0 & 0 & \cdots & 0 & 0\\
0 & -\frac{1}{h^2} & \frac{2}{h^2} + V_3 & -\frac{1}{h^2} & 0 & \cdots & 0 & 0\\
\vdots & & & \ddots & & & & \vdots\\
0 & 0 & \cdots & 0 & -\frac{1}{h^2} & \frac{2}{h^2} + V_{n-2} & -\frac{1}{h^2} & 0\\
0 & 0 & \cdots & 0 & 0 & -\frac{1}{h^2} & \frac{2}{h^2} + V_{n-1} & -\frac{1}{h^2}\\
0 & 0 & \cdots & 0 & 0 & 0 & -\frac{1}{h^2} & \frac{2}{h^2} + V_{n}\\
\end{bmatrix}
\begin{bmatrix}
\psi_1\\
\psi_2\\
\psi_3\\
\vdots\\
\psi_{n-2}\\
\psi_{n-1}\\
\psi_n
\end{bmatrix}
=
\lambda
\begin{bmatrix}
\psi_1\\
\psi_2\\
\psi_3\\
\vdots\\
\psi_{n-2}\\
\psi_{n-1}\\
\psi_n
\end{bmatrix}
\]
If we define \(A\) as the matrix above and \(\vec{\psi}\) as \((\psi_1,\psi_2,\dots,\psi_n)\), we have an eigenvalue problem
\[
A\vec{\psi}=\lambda\vec{\psi}
\]

%   __  __       _   _                          _   _           _   _   _
%  |  \/  | __ _| |_| |__   ___ _ __ ___   __ _| |_(_) ___ __ _| | | |_| |__   ___  ___  _ __ _   _
%  | |\/| |/ _` | __| '_ \ / _ \ '_ ` _ \ / _` | __| |/ __/ _` | | | __| '_ \ / _ \/ _ \| '__| | | |
%  | |  | | (_| | |_| | | |  __/ | | | | | (_| | |_| | (_| (_| | | | |_| | | |  __/ (_) | |  | |_| |
%  |_|  |_|\__,_|\__|_| |_|\___|_| |_| |_|\__,_|\__|_|\___\__,_|_|  \__|_| |_|\___|\___/|_|   \__, |
%                                                                                             |___/

\section{Mathematical theory}


\subsection{Orthogonal transformations of vectors}
Jacobi's method relies heavily on orthogonal transformations and their properties. Orthogonal transformations are, in \(\R^n\), functions on the form
\[
T:\vec{x}\mapsto U\vec{x}
\]
where \(\vec{x}\) is a vector in \(\R^n\) and \(U\) is a real, orthogonal \(n\times n\) matrix.

If \(U\) is orthogonal, \(U^TU=I\). This implies:
\[
T(\vec{v}_i)\cdot T(\vec{v}_j) = \qty(U\vec{v}_i)\cdot\qty(U\vec{v}_j) = \qty(U\vec{v}_i)^T\qty(U\vec{v}_j) = \vec{v}_i^T\overbrace{U^TU}^I\vec{v}_j = \vec{v}_i^T\vec{v}_j=\vec{v}_i\cdot \vec{v}_j
\]
Orthogonal transformations hence conserve the inner product, and therefore also the orthogonality.



\subsection{Orthogonal similarity transformations of matrices}
If we have a matrix \(A\) with an eigenvector \(\vec{x}\) with corresponding eigenvalue \(\lambda\), then
\begin{alignat*}{2}
A\vec{x} &= \lambda \vec{x}
\intertext{Multiplying with the transpose of an orthogonal matrix \(S\), we get}
S^TA\vec{x} &= \lambda S^T\vec{x}
\intertext{As \(S\) is orthogonal, \(SS^T=I\), so we can sneak in an identity matrix disguised as \(SS^T\) between \(A\) and \(\vec{x}\) on the left hand side:}
S^TASS^T\vec{x} &= \lambda S^T\vec{x}\\
\qty(S^TAS)\qty(S^T\vec{x}) &= \lambda \qty(S^T\vec{x}) \eqtag{eigenvectransform}
\end{alignat*}
This proves that if \(A\) is transformed to \(S^TAS\), the eigenvalues remain the same, while the new eigenvector is \(S^T\vec{x}\).

It is easy to prove that symmetry is conserved by the similarity transformation:
\[
\qty(S^TAS)^T = S^TA^T\qty(S^T)^T = S^TA^TS = S^TAS
\]
if and only if \(A^T=A\), i.e. when \(A\) is symmetric.

It can also be shown that the Frobenius norm, the square root of the sum of the squares of the matrix elements, is also conserved by the transformation\autocite{compphys}.

\subsection{Strategy for finding eigenvalues and eigenvectors, using orthogonal similarity transformations}
If the transformation matrix \(S\) is chosen in a special way such that the largest non-diagonal element of \(A\) is transformed to \(0\), the sum of the squares of the non-diagonal elements decreases from the transformations. This means that if the transformation is applied repeatedly, the matrix is converging towards a diagonal matrix \(D\).

If \(D=\operatorname{diag}\qty{d_1,d_2,\dots,d_n}\), the characteristic polynomial is given by
\[
\operatorname{det}\qty(\lambda I - D) = \qty(\lambda-d_1)\cdot\qty(\lambda-d_2)\cdot\ldots\cdot\qty(\lambda-d_n)
\]
and hence the eigenvalues of \(D\) are simply the diagonal elements. The eigenvectors can be chosen as the elements in the standard basis for \(\R^n\), i.e. the columns of the identity matrix.

When \(A\) is transformed to \(D\), the matrix of eigenvectors, \(R\), is therefore transformed to \(I\). On the other hand, we know from \vref{eigenvectransform} that if \(\vec{x}\) is an eigenvector of \(A\), then \(S^T\vec{x}\) is an eigenvector of the transformed matrix. If \(A\) is transformed into \(D\) after \(n\) transformations with the transformation matrices \(S_1,S_2,\dots,S_n\), then each eigenvector \(\vec{x}\) is transformed to \(S_n^TS_{n-1}^T\dots S_1^T\vec{x}\). We must therefore have
\[
I = S_n^TS_{n-1}^T\dots S_1^TR \implies R = S_1S_2\dots S_nI = IS_1S_2\dots S_n \eqtag{findR}
\]

The conclusion is that if a suitable transformation matrix can be chosen, we have a method for finding both the eigenvalues and the eigenvectors of any symmetric matrix.

%    ____ _                     _               _   _
%   / ___| |__   ___   ___  ___(_)_ __   __ _  | |_| |__   ___
%  | |   | '_ \ / _ \ / _ \/ __| | '_ \ / _` | | __| '_ \ / _ \
%  | |___| | | | (_) | (_) \__ \ | | | | (_| | | |_| | | |  __/
%   \____|_| |_|\___/ \___/|___/_|_| |_|\__, |  \__|_| |_|\___|_
%  | |_ _ __ __ _ _ __  ___ / _| ___  _ |___/__ ___   __ _| |_(_) ___  _ __
%  | __| '__/ _` | '_ \/ __| |_ / _ \| '__| '_ ` _ \ / _` | __| |/ _ \| '_ \
%  | |_| | | (_| | | | \__ \  _| (_) | |  | | | | | | (_| | |_| | (_) | | | |
%   \__|_|  \__,_|_|_|_|___/_|  \___/|_|  |_| |_| |_|\__,_|\__|_|\___/|_| |_|
%   _ __ ___   __ _| |_ _ __(_)_  __
%  | '_ ` _ \ / _` | __| '__| \ \/ /
%  | | | | | | (_| | |_| |  | |>  <
%  |_| |_| |_|\__,_|\__|_|  |_/_/\_\



\subsection{Choosing the transformation matrix}
Jacobi discovered that one could choose a matrix \(S\) which is identical to the identity apart from the following elements:
\[
s_{kk} = s_{ll} = \cos(\theta), \quad s_{kl} = \sin(\theta), \quad s_{lk} = \sin(\theta)
\]
where \(\qty(k,l)\) are the indices of the largest non-diagonal element of the matrix to be transformed, and \(\theta\) is to be determined in such a way that the largest non-diagonal elements are transformed to \(0\).

With \(B=S^TAS\), all elements apart from those in row or column \(k\) and \(l\) are left unchanged. The other elements are transformed as follows:
\begin{alignat*}{2}
b_{ik} &= b_{ki} = a_{ik}\cos(\theta) - a_{il}\sin(\theta), \quad i\neq k, i\neq l\\
b_{il} &= b_{li} = a_{il}\cos(\theta) + a_{ik}\sin(\theta),\quad i\neq k, i\neq l\\
b_{kk} &= a_{kk}\cos[2](\theta) - 2a_{kl}\cos(\theta)\sin(\theta) + a_{ll}\sin[2](\theta)\\
b_{ll} &= a_{ll}\cos[2](\theta) + 2a_{kl}\cos(\theta)\sin(\theta) + a_{kk}\sin[2](\theta)\\
b_{kl} &= b_{lk} = \qty(a_{kk} - a_{ll})\cos(\theta)\sin(\theta) + a_{kl}\qty(\cos[2](\theta)-\sin[2](\theta))
\end{alignat*}
Additionally, we know from \vref{findR} that the new eigenvector matrix is transformed by multiplying by \(S\) from the right hand side, which yields the matrix \(\tilde{R}\). All elements are left unchanged, apart from
\begin{alignat*}{2}
\tilde{r}_{ik} &= r_{ik}\cos(\theta) - r_{il}\sin(\theta)\\
\tilde{r}_{il} &= r_{ik}\sin(\theta) + r_{il}\cos(\theta)
\end{alignat*}
We must now choose \(\theta\) such that \(a_{kl}\), the largest non-diagonal element, is transformed to \(0\). This means solving the equation \(b_{kl}=0\):
 \begin{alignat*}{2}
 0 &= b_{kl} = \qty(a_{kk} - a_{ll})\cos(\theta)\sin(\theta) + a_{kl}\qty(\cos[2](\theta)-\sin[2](\theta))
\end{alignat*}
This leads to
\begin{alignat*}{2}
\qty(a_{ll}-a_{kk})\cos(\theta)\sin(\theta) &= a_{kl}\qty(\cos[2](\theta)-\sin[2](\theta))\\
\frac{a_{ll}-a_{kk}}{a_{kl}} &=\frac{\cos[2](\theta)-\sin[2](\theta)}{\cos(\theta)\sin(\theta)}
\intertext{Defining the left hand side as \(2\tau\) and dividing by \(\cos[2](\theta)\) in the numerator and denominator on the right hand side, we get}
2\tau &=\frac{1-\tan[2](\theta)}{\tan(\theta)}\\
0 &= \tan[2](\theta)-2\tau\tan(\theta)-1\\
\tan(\theta) &= -\tau\pm\sqrt{\tau^2+1}
\end{alignat*}
Either sign in front of the square root can in principle be chosen. Numerically, however, there is a difference. The point of the algorithm is to get the non-diagonal elements, of which \(a_{kl}\) is the largest, as small as possible, which means \(\tau\) will grow larger for every iteration. When \(\tau\) is large, \(\sqrt{\tau^2+1}\approx\abs{\tau}\).

If \(\tau\) is positive and the \(+\) sign is chosen (or vice versa), a large, negative number and a large, positive number are added and give approximately \(0\). If for example the computer is able to store \(8\) digits in the mantissa and the result is \(6\) orders of magnitudes smaller, the result will only have \(2\) accurate digits.

The conclusion is that if \(\tau\) is negative, the \(+\) sign should be chosen, while if \(\tau\) is positive, the \(-\) sign should be chosen.

\(\cos(\theta)\) and \(\sin(\theta)\) can then be calculated from the formulas
\[
\cos(\theta)=\frac{1}{\sqrt{1+\tan[2](\theta)}}\qquad\qquad \sin(\theta)=\tan(\theta)\cos(\theta)
\]

\clearpage
\subsection{Implementation of the algorithm}
The complete function is available on GitHub\footnote{\url{https://github.com/anjohan/Offentlig/blob/master/FYS3150/Oblig2/jacobi.cpp}}.
\lstinputlisting[language=C++,linerange={jacobistart-jacobiend},caption={Implementation of Jacobi's method in C++.}]{jacobi.cpp}



%   ____                 _ _                         _       _ _                        _
%  |  _ \ ___  ___ _   _| | |_ ___    __ _ _ __   __| |   __| (_)___  ___ _   _ ___ ___(_) ___  _ __
%  | |_) / _ \/ __| | | | | __/ __|  / _` | '_ \ / _` |  / _` | / __|/ __| | | / __/ __| |/ _ \| '_ \
%  |  _ <  __/\__ \ |_| | | |_\__ \ | (_| | | | | (_| | | (_| | \__ \ (__| |_| \__ \__ \ | (_) | | | |
%  |_| \_\___||___/\__,_|_|\__|___/  \__,_|_| |_|\__,_|  \__,_|_|___/\___|\__,_|___/___/_|\___/|_| |_|
%


\section{Results and discussion}
\subsection{Simulation for one particle}
\begin{figure}[H]
\centering
\input{plot1.tex}
\caption{Simulation of one particle in a harmonic oscillator potential for \(n=250\) with \(\rho_{\max}=8\).}\label{fig:en}
\end{figure}
We see that when \(\lambda\), and therefore also the energy, increases, the particle is able to move further out from the minimum of the harmonic oscillator well. This fits well with the harmonic oscillator from classical mechanics. Additionally, the number of nodes increases as \(\lambda\) increases, which is a trend also found when solving for example the quantum mechanical particle-in-a-box problem.

\begin{table}[H]
\[
\input{egenverdier.dat}
\]
\caption{The three lowest eigenvalues found by the algorithm for the different number of mesh points \(n\). The analytical values are \(\lambda_0=3\), \(\lambda_1=7\) and \(\lambda_2=11\).}
\end{table}
The number of iterations is approximately quadrupled when the number of mesh points is doubled, indicating that the numbers of iterations runs as \(n^2\). This means that the number of iterations is proportional to the number of elements in the matrix. However, the number of non-diagonal elements in the tridiagonal elements handled in this project is proportional to \(n\), so this trend is unlikely to hold for dense matrices.

When \(n\) is a few hundred, the runtime is already noticeable, implying that Jacobi's method is not well suited for finding the eigenvalues and eigenvectors for large systems. Note that the times listed include setting up the matrices etc. As \(n\) is doubled, e.g. from \(100\) to \(200\) or from \(200\) to \(400\), the time spent is multiplied with roughly \(16\), indicating that the time runs as \(n^4\).


\subsection{Simulation for two particles}
\begin{figure}[H]
\centering
\input{plot2.tex}
\caption{Simulation of two particles, with and without Coulomb interaction, for two different values of \(\omega_r\). A larger \(\omega_r\) corresponds to a stronger harmonic oscillator.}\label{fig:to}
\end{figure}

Without interaction and with \(\omega_r=1\), \ref{slen} and \ref{slutento}, become identical, and we see from \vref{fig:en} and \vref{fig:to} that the algorithm finds the same eigenstates.

The strength of the harmonic oscillator is represented by \(\omega_r\). When \(\omega_r\) is large, it is therefore expected that the particles are pulled closer to origo so that \(\rho\) decreases, and that their position is more sharply determined. This fits very well with the figure.

Because the interaction between two electrons is repulsive, the particles are further apart when the interaction is taken into account. The effect of this is greater when \(\omega_r\) is smaller and the particles are more free to move around, as the Coulomb interaction is a far reaching force.

%  _____         _
% |_   _|__  ___| |_ ___
%   | |/ _ \/ __| __/ __|
%   | |  __/\__ \ |_\__ \
%   |_|\___||___/\__|___/
%

\section{Tests}
The code is found at GitHub\footnote{\url{https://github.com/anjohan/Offentlig/blob/master/FYS3150/Oblig2/test.cpp}}.
\subsection{Ability to find eigenvalues and eigenvectors}
The matrix
\[
A = \begin{bmatrix}7 & -2 & 0\\ -2 & 6 & -2\\ 0 & -2 & 5\end{bmatrix}
\]
has the pretty eigenvalues \(3\), \(6\) and \(9\), with corresponding eigenvectors
\[
\begin{bmatrix}1 \\ 2 \\ 2\end{bmatrix},\quad
\begin{bmatrix}-2 \\ 1 \\ 2\end{bmatrix}, \quad
\begin{bmatrix}2 \\ -2 \\ 1\end{bmatrix}
\]

The implementation of Jacobi's method can be tested on this matrix using
\lstinputlisting[language=C++,linerange={eigenteststart-eigentestslutt},caption={Test of eigenvalues.}]{test.cpp}
where \texttt{A} and \texttt{R} are \(3\times3\) pointer matrices and \texttt{n} is \(3\). Formatted printing of the resulting \texttt{A} and \texttt{R} yields
\lstinputlisting[frame=none,language=]{eigentest.txt}
This is the expected result, except that the program returns normalized eigenvectors. The second printed matrix has columns of eigenvectors corresponding to different eigenvalues for a diagonal matrix, so the columns should be orthogonal. Simple arithmetic shows that this is correct.


\clearpage
\subsection{Ability to find largest non-diagonal elements}
A simple test is
\lstinputlisting[language=C++,linerange={largeststart-largestend},caption={Test of search for largest non-diagonal element.}]{test.cpp}
which yields
\lstinputlisting[frame=none,language=]{storstetest.txt}
This is correct, as the function searches for the element with the largest absolute value. It is not necessary to specify the upper triangle of the matrix, as the function is specialized for symmetric matrices and therefore only searches the lower triangle.


%    ____                 _           _
%   / ___|___  _ __   ___| |_   _ ___(_) ___  _ __
%  | |   / _ \| '_ \ / __| | | | / __| |/ _ \| '_ \
%  | |__| (_) | | | | (__| | |_| \__ \ | (_) | | | |
%   \____\___/|_| |_|\___|_|\__,_|___/_|\___/|_| |_|
%

\section{Conclusion}
In this project, Jacobi's algorithm was successfully implemented, and it proved sufficiently efficient to be able for solving Schrödinger's equation for several situations. The run time was, however, significant for as little as a few hundred steps, indicating that Jacobi's method is generally not very well suited for large systems. For much larger \(n\), it would be beneficial to instead implement either Householder's method or Lanczos' method. When the three point formula is used to approximate the second derivative, the resulting matrix is tridiagonal, so it would probably be wise to try and utilise this fact and its consequences in developing a more specialized algorithm (as was done with great success in project 1, see \autocite{oblig1}).




\clearpage
\addcontentsline{toc}{section}{References}
\printbibliography


\end{document}
