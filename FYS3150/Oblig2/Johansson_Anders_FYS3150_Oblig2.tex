\documentclass[12pt,english,a4paper]{report}
\pdfobjcompresslevel=0
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[includeheadfoot,margin=0.8 in,top=0.6 in]{geometry}
\usepackage{siunitx,physics,cancel,upgreek,varioref,listings,booktabs,tocloft, pdfpages}
\usepackage{mathtools}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{fouriernc}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{lastpage}
\usepackage{microtype}
\usepackage[linktoc=all, bookmarks=true, pdfauthor={Anders Johansson}]{hyperref}
\renewcommand{\CancelColor}{\color{red}}
\renewcommand{\exp}[1]{\mathrm{e}^{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\tittel}[1]{\title{#1 \vspace{-7ex}}\author{}\date{}\maketitle\thispagestyle{fancy}\pagestyle{fancy}\setcounter{page}{1}}

\newcommand{\deloppg}[2][]{\subsection*{#2) #1}\addcontentsline{toc}{subsection}{#2)}\refstepcounter{subsection}\label{#2}}
\newcommand{\oppg}[1]{\section*{Oppgave #1}\addcontentsline{toc}{section}{Oppgave #1}\refstepcounter{section}\label{oppg#1}}

\labelformat{section}{section~#1}
\labelformat{subsection}{section~#1}
\labelformat{subsubsection}{paragraph~#1}
\labelformat{equation}{equation~(#1)}
\labelformat{figure}{figure~#1}
\labelformat{table}{table~#1}

\lstset{rangeprefix=/*\#,
rangesuffix=\#*/,
includerangemarker=false}
\renewcommand{\lstlistingname}{Code snippet}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstset{showstringspaces=false,
basicstyle=\footnotesize\ttfamily,
keywordstyle=\color{codegreen},
commentstyle=\color{magenta},
numberstyle=\tiny\color{codegray},
stringstyle=\color{codepurple},
frameshape={RYRYNYYYY}{yny}{yny}{RYRYNYYYY},
breaklines=true,
literate={0}{{\textcolor{blue}{0}}}{1}%
             {1}{{\textcolor{blue}{1}}}{1}%
             {2}{{\textcolor{blue}{2}}}{1}%
             {3}{{\textcolor{blue}{3}}}{1}%
             {4}{{\textcolor{blue}{4}}}{1}%
             {5}{{\textcolor{blue}{5}}}{1}%
             {6}{{\textcolor{blue}{6}}}{1}%
             {7}{{\textcolor{blue}{7}}}{1}%
             {8}{{\textcolor{blue}{8}}}{1}%
             {9}{{\textcolor{blue}{9}}}{1}%
             {.0}{{\textcolor{blue}{.0}}}{2}% Following is to ensure that only periods
             {.1}{{\textcolor{blue}{.1}}}{2}% followed by a digit are changed.
             {.2}{{\textcolor{blue}{.2}}}{2}%
             {.3}{{\textcolor{blue}{.3}}}{2}%
             {.4}{{\textcolor{blue}{.4}}}{2}%
             {.5}{{\textcolor{blue}{.5}}}{2}%
             {.6}{{\textcolor{blue}{.6}}}{2}%
             {.7}{{\textcolor{blue}{.7}}}{2}%
             {.8}{{\textcolor{blue}{.8}}}{2}%
             {.9}{{\textcolor{blue}{.9}}}{2}%
}

\renewcommand{\footrulewidth}{\headrulewidth}
\tocloftpagestyle{fancy}

\setcounter{secnumdepth}{4}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\arabic{section}.\arabic{subsection}.\arabic{subsubsection}}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1em}

\newcommand{\eqtag}[1]{\refstepcounter{equation}\tag{\theequation}\label{#1}}
\hypersetup{colorlinks=true,urlcolor=blue,linkcolor=black}

\sisetup{detect-all}
\sisetup{exponent-product = \cdot, output-product = \cdot,per-mode=symbol}
\sisetup{output-decimal-marker={.}}
\sisetup{round-mode = off, round-precision=3}
\sisetup{number-unit-product = \ }

\allowdisplaybreaks[4]
\fancyhf{}

\rhead{Anders Johansson}
\rfoot{Page \thepage{} of \pageref{LastPage}}
\lhead{FYS3150}
%
\usepackage[backend=biber,citestyle=numeric-comp,bibstyle=numeric,sorting=none]{biblatex}
\DefineBibliographyStrings{norsk}{%
  bibliography = {Referanser},
}
\addbibresource{kilder.bib}

\begin{document}
%\includepdf{forside.pdf}
\pagestyle{fancy}
\tableofcontents

\section{Abstract}

All files for this project are available at GitHub\footnote{\url{https://github.com/anjohan/Offentlig/tree/master/FYS3150/Oblig2}}.


\section{Introduction}





\section{Theory}





\subsection{Orthogonal transformations of vectors}
Jacobi's method relies heavily on orthogonal transformations and their properties. Orthogonal transformations are, in \(\R^n\), functions on the form
\[
T:\vec{x}\mapsto U\vec{x}
\]
where \(\vec{x}\) is a vector in \(\R^n\) and \(U\) is a real, orthogonal \(n\times n\) matrix.

If \(U\) is orthogonal, \(U^TU=I\). This implies:
\[
T(\vec{v}_i)\cdot T(\vec{v}_j) = \qty(U\vec{v}_i)\cdot\qty(U\vec{v}_j) = \qty(U\vec{v}_i)^T\qty(U\vec{v}_j) = \vec{v}_i^T\overbrace{U^TU}^I\vec{v}_j = \vec{v}_i^T\vec{v}_j=\vec{v}_i\cdot \vec{v}_j
\]
Orthogonal transformations hence conserve the inner product, and therefore also the orthogonality.



\subsection{Orthogonal similarity transformations of matrices}
If we have a matrix \(A\) with an eigenvector \(\vec{x}\) with corresponding eigenvalue \(\lambda\), then
\begin{alignat*}{2}
A\vec{x} &= \lambda \vec{x}
\intertext{Multiplying with the transpose of an orthogonal matrix \(S\), we get}
S^TA\vec{x} &= \lambda S^T\vec{x}
\intertext{As \(S\) is orthogonal, \(SS^T=I\), so we can sneak in an identity matrix disguised as \(SS^T\) between \(A\) and \(\vec{x}\) on the left hand side:}
S^TASS^T\vec{x} &= \lambda S^T\vec{x}\\
\qty(S^TAS)\qty(S^T\vec{x}) &= \lambda \qty(S^T\vec{x}) \eqtag{eigenvectransform}
\end{alignat*}
This proves that if \(A\) is transformed to \(S^TAS\), the eigenvalues remain the same, while the new eigenvector is \(S^T\vec{x}\).

It is easy to prove that symmetry is conserved by the similarity transformation:
\[
\qty(S^TAS)^T = S^TA^T\qty(S^T)^T = S^TA^TS = S^TAS
\]
if and only if \(A^T=A\), i.e. when \(A\) is symmetric.

It can also be shown that the Frobenius norm, the square root of the sum of the squares of the matrix elements, is also conserved by the transformation\autocite{compphys}.

\subsection{Strategy for finding eigenvalues and eigenvectors, using orthogonal similarity transformations}
If the transformation matrix \(S\) is chosen in a special way such that the largest non-diagonal element of \(A\) is transformed to \(0\), the sum of the squares of the non-diagonal elements decreases from the transformations. This means that if the transformation is applied repeatedly, the matrix is converging towards a diagonal matrix \(D\).

If \(D=\operatorname{diag}\qty{d_1,d_2,\dots,d_n}\), the characteristic polynomial is given by
\[
\operatorname{det}\qty(\lambda I - D) = \qty(\lambda-d_1)\cdot\qty(\lambda-d_2)\cdot\ldots\cdot\qty(\lambda-d_n)
\]
and hence the eigenvalues of \(D\) are simply the diagonal elements. The eigenvectors can be chosen as the elements in the standard basis for \(\R^n\), i.e. the columns of the identity matrix.

When \(A\) is transformed to \(D\), the matrix of eigenvectors, \(R\), is therefore transformed to \(I\). On the other hand, we know from \vref{eigenvectransform} that if \(\vec{x}\) is an eigenvector of \(A\), then \(S^T\vec{x}\) is an eigenvector of the transformed matrix. If \(A\) is transformed into \(D\) after \(n\) transformations with the transformation matrices \(S_1,S_2,\dots,S_n\), then each eigenvector \(\vec{x}\) is transformed to \(S_n^TS_{n-1}^T\dots S_1^T\vec{x}\). We must therefore have
\[
I = S_n^TS_{n-1}^T\dots S_1^TR \implies R = S_1S_2\dots S_nI = IS_1S_2\dots S_n \eqtag{findR}
\]

The conclusion is that if a suitable transformation matrix can be chosen, we have a method for finding both the eigenvalues and the eigenvectors of any symmetric matrix.

\subsection{Choosing the transformation matrix}
Jacobi discovered that one could choose a matrix \(S\) which is identical to the identity apart from the following elements:
\[
s_{kk} = s_{ll} = \cos(\theta), \quad s_{kl} = \sin(\theta), \quad s_{lk} = \sin(\theta)
\]
where \(\qty(k,l)\) are the indices of the largest non-diagonal element of the matrix to be transformed, and \(\theta\) is to be determined in such a way that the largest non-diagonal elements are transformed to \(0\).

With \(B=S^TAS\), all elements apart from those in row or column \(k\) and \(l\) are left unchanged. The other elements are transformed as follows:
\begin{alignat*}{2}
b_{ik} &= b_{ki} = a_{ik}\cos(\theta) - a_{il}\sin(\theta), \quad i\neq k, i\neq l\\
b_{il} &= b_{li} = a_{il}\cos(\theta) + a_{ik}\sin(\theta),\quad i\neq k, i\neq l\\
b_{kk} &= a_{kk}\cos[2](\theta) - 2a_{kl}\cos(\theta)\sin(\theta) + a_{ll}\sin[2](\theta)\\
b_{ll} &= a_{ll}\cos[2](\theta) + 2a_{kl}\cos(\theta)\sin(\theta) + a_{kk}\sin[2](\theta)\\
b_{kl} &= b_{lk} = \qty(a_{kk} - a_{ll})\cos(\theta)\sin(\theta) + a_{kl}\qty(\cos[2](\theta)-\sin[2](\theta))
\end{alignat*}
Additionally, we know from \vref{findR} that the new eigenvector matrix is transformed by multiplying by \(S\) from the right hand side, which yields the matrix \(\tilde{R}\). All elements are left unchanged, apart from
\begin{alignat*}{2}
\tilde{r}_{ik} &= r_{ik}\cos(\theta) - r_{il}\sin(\theta)\\
\tilde{r}_{il} &= r_{ik}\sin(\theta) + r_{il}\cos(\theta)
\end{alignat*}
We must now choose \(\theta\) such that \(a_{kl}\), the largest non-diagonal element, is transformed to \(0\). This means solving the equation \(b_{kl}=0\):
 \begin{alignat*}{2}
 0 &= b_{kl} = \qty(a_{kk} - a_{ll})\cos(\theta)\sin(\theta) + a_{kl}\qty(\cos[2](\theta)-\sin[2](\theta))
\end{alignat*}
This leads to
\begin{alignat*}{2}
\qty(a_{ll}-a_{kk})\cos(\theta)\sin(\theta) &= a_{kl}\qty(\cos[2](\theta)-\sin[2](\theta))\\
\frac{a_{ll}-a_{kk}}{a_{kl}} &=\frac{\cos[2](\theta)-\sin[2](\theta)}{\cos(\theta)\sin(\theta)}
\intertext{Defining the left hand side as \(2\tau\) and dividing by \(\cos[2](\theta)\) in the numerator and denominator on the right hand side, we get}
2\tau &=\frac{1-\tan[2](\theta)}{\tan(\theta)}\\
0 &= \tan[2](\theta)-2\tau\tan(\theta)-1\\
\tan(\theta) &= -\tau\pm\sqrt{\tau^2+1}
\end{alignat*}
Either sign in front of the square root can in principle be chosen. Numerically, however there is a difference. The point of the algorithm is to get the non-diagonal elements, of which \(a_{kl}\) is the largest, as small as possible, which means \(\tau\) will grow larger for every iteration. When \(\tau\) is large, \(\sqrt{\tau^2+1}\approx\abs{\tau}\).

If \(\tau\) is positive and the \(+\) sign is chosen (or vice versa), a large, negative number and a large, positive number are added and give approximately \(0\). If for example the computer is able to store \(8\) digits in the mantissa and the result is \(6\) orders of magnitudes smaller, the result will only have \(2\) accurate digits.

The conclusion is that if \(\tau\) is negative, the \(+\) sign should be chosen, while if \(\tau\) is positive, the \(-\) sign should be chosen.

\(\cos(\theta)\) and \(\sin(\theta)\) can then be calculated from the formulas
\[
\cos(\theta)=\frac{1}{\sqrt{1+\tan[2](\theta)}}\qquad\qquad \sin(\theta)=\tan(\theta)\cos(\theta)
\]

\clearpage
\subsection{Implementation of the algorithm}
\lstinputlisting[language=C++,linerange={jacobistart-jacobiend}]{jacobi.cpp}






\section{Results and discussion}
\subsection{Simulation for one particle}
\begin{figure}[H]
\centering
\input{plot1.tex}
\caption{Simulation of one particle in a harmonic oscillator potential for \(n=250\) with \(\rho_{\max}=8\).}
\end{figure}

\begin{table}[H]
\[
\input{egenverdier.dat}
\]
\caption{The three lowest eigenvalues found by the algorithm for the different number of mesh points \(n\). The analytical values are \(\lambda_0=3\), \(\lambda_1=7\) and \(\lambda_2=11\).}
\end{table}
The number of iterations is approximately quadrupled when the number of mesh points is doubled, indicating that the numbers of iterations runs as \(n^2\). This means that the number of iterations is proportional to the number of elements in the matrix. Note that the matrices handled in this project are tridiagonal, and the trends may be different for dense matrices.


\subsection{Simulation for two particles}
\begin{figure}[H]
\centering
\input{plot2.tex}
\caption{Simulation of two particles, with and without Coulomb interaction, for two different values of \(\omega\). A larger \(\omega\) corresponds to a stronger harmonic oscillator.}
\end{figure}


\section{Tests}
The code is found at GitHub\footnote{\url{https://github.com/anjohan/Offentlig/blob/master/FYS3150/Oblig2/test.cpp}}.
\subsection{Ability to find eigenvalues and eigenvectors}
The matrix
\[
A = \begin{bmatrix}7 & -2 & 0\\ -2 & 6 & -2\\ 0 & -2 & 5\end{bmatrix}
\]
has the pretty eigenvalues \(3\), \(6\) and \(9\), with corresponding eigenvectors
\[
\begin{bmatrix}1 \\ 2 \\ 2\end{bmatrix},\quad
\begin{bmatrix}-2 \\ 1 \\ 2\end{bmatrix}, \quad
\begin{bmatrix}2 \\ -2 \\ 1\end{bmatrix}
\]

The implementation of Jacobi's method can be tested on this matrix using
\lstinputlisting[language=C++,linerange={eigenteststart-eigentestslutt},caption={Test of eigenvalues.}]{test.cpp}
where \texttt{A} and \texttt{R} are \(3\times3\) pointer matrices and \texttt{n} is \(3\). Formatted printing of the resulting \texttt{A} and \texttt{R} yields
\lstinputlisting[frame=none,language=]{eigentest.txt}
This is the expected result, except that the program returns normalized eigenvectors.

\subsection{Ability to find largest non-diagonal elements}
A simple test is
\lstinputlisting[language=C++,linerange={largeststart-largestend},caption={Test of search for largest non-diagonal element.}]{test.cpp}
which yields
\lstinputlisting[frame=none,language=]{storstetest.txt}
This is correct, as the function searches for the element with the largest absolute value. It is not necessary to specify the upper triangle of the matrix, as the function is specialized for symmetric matrices and therefore only searches the lower triangle.








\clearpage
\addcontentsline{toc}{section}{References}
\printbibliography


\end{document}
